{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7504480,"sourceType":"datasetVersion","datasetId":4370214}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dipds109/ola-smote-bagging-boosting?scriptVersionId=160922489\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# OLA Driver Churn\n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing the data","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('ola_driver_scaler.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the couloms are numerical columns except the city column, which contains the city codes where the driver is from.","metadata":{}},{"cell_type":"code","source":"df['Driver_ID'].nunique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Grouping by the driver column we can see that there are unique 2381 driver Ids.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we haev 61 missing age and 52 gender null values, We willaddress this but b4 that let us convert the date columns to the proper datatypes.","metadata":{}},{"cell_type":"code","source":"date_columns = [\"MMM-YY\", \"Dateofjoining\", \"LastWorkingDate\"]\n\nfor column in date_columns:\n    df[column] = pd.to_datetime(df[column])\n\nprint(df.dtypes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### KNN Imputation to fill up the numerical couloms","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import KNNImputer\n\nnumerical_columns = ['Driver_ID', 'Age', 'Education_Level', 'Income', 'Joining Designation', 'Grade', 'Total Business Value', 'Quarterly Rating']\nnumerical_data = df[numerical_columns]\n\nimputer = KNNImputer(n_neighbors=5)  # You can adjust the number of neighbors as needed\nimputed_numerical_data = imputer.fit_transform(numerical_data)\nimputed_numerical_df = pd.DataFrame(imputed_numerical_data, columns=numerical_columns)\n\ndf[numerical_columns] = imputed_numerical_df\n\n# Verify that missing values have been imputed\nmissing_values = df.isnull().sum()\nprint(\"Missing values after imputation:\")\nprint(missing_values)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":" Creating a column named Quarterly_Rating_Change which targets the drivers whose performance have imroved over time.","metadata":{}},{"cell_type":"code","source":"df['Quarterly_Rating_Change'] = df.groupby('Driver_ID')['Quarterly Rating'].diff()\ndf['Rating_Increased'] = (df['Quarterly_Rating_Change'] > 0).astype(int)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a column named target. The drivers where the last working date is not mentioned are believed to have not left the origanization and are alloted a target value of 0. Drivers who left the org are alloted a target variable 1. ","metadata":{}},{"cell_type":"code","source":"df['target']=0\ndf.loc[df['LastWorkingDate'].notnull(), 'target'] = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a column named Income_Diff, for the drivers who income has increased overtime.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Sort the dataset by 'Driver_ID' and 'MMM-YY' to ensure the data is in the correct order\ndf.sort_values(by=['Driver_ID', 'MMM-YY'], inplace=True)\n\n# Create a new column 'Income_Increased' with default value 0\ndf['Income_Increased'] = 0\n\n# Calculate the difference in income for each driver\ndf['Income_Diff'] = df.groupby('Driver_ID')['Income'].diff()\n\n# Set 'Income_Increased' value to 1 for drivers whose income has increased\ndf.loc[df['Income_Diff'] > 0, 'Income_Increased'] = 1\n\n# Drop the 'Income_Diff' column if you no longer need it\ndf.drop(columns=['Income_Diff'], inplace=True)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally creating a column to check the total number of days the driver is associated with the organization since their time of joining. The dataset is for the years 2019-20 so i have taken the last day as 12/12/2020. I have calculated the total working days with reference to this day.","metadata":{}},{"cell_type":"code","source":"df['Dateofjoining']=pd.to_datetime(df['Dateofjoining'])\ndf['LastWorkingDate']=pd.to_datetime(df['LastWorkingDate'])\n\n# Use '12/12/2020' as the default date for LastWorkingDate where it's None\ndefault_date = pd.to_datetime('12/12/2020')\ndf['LastWorkingDate'].fillna(default_date, inplace=True)\n\n# Calculate the difference and store it in 'total_num_of_days' column\ndf['total_num_of_days'] = (df['LastWorkingDate'] - df['Dateofjoining']).dt.days\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking whether all the couloms are visible properly or not.","metadata":{}},{"cell_type":"code","source":"df.head(8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating an aggregated data by grouping the data with driver ids. This was done as there was multiple entries for each driver.","metadata":{}},{"cell_type":"code","source":"aggregated_data = df.groupby('Driver_ID').agg({\n    'Age': 'last',\n    'Gender': 'max',\n    'City': 'last',\n    'Education_Level': 'last',\n    'Income': 'last',\n    'Dateofjoining': 'first',\n    'LastWorkingDate': 'last',\n    'Joining Designation': 'first',\n    'Grade': 'last',\n    'Total Business Value': 'sum',\n    'Quarterly Rating': 'last',\n    'target':'last',\n    'Rating_Increased':'max',\n    'Income_Increased':'max',\n    'total_num_of_days':'max'\n    \n}).reset_index()\n\n# Displaying the first few rows of the aggregated data\naggregated_data.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Univariate Analysis","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming 'df' is your DataFrame containing the dataset\n\n# List of continuous variables\ncontinuous_vars = ['Age', 'Income', 'Total Business Value','total_num_of_days']\n\n# Create distribution plots for continuous variables\nfor var in continuous_vars:\n    plt.figure(figsize=(8, 4))\n    \n    # Plot the distribution\n    sns.histplot(aggregated_data[var], kde=True)\n    \n    # Calculate and add average line\n    average = aggregated_data[var].mean()\n    plt.axvline(average, color='red', linestyle='--', label='Average')\n    \n    # Calculate and add 25% and 75% lines\n    percentile_25 = aggregated_data[var].quantile(0.25)\n    percentile_75 = aggregated_data[var].quantile(0.75)\n    plt.axvline(percentile_25, color='green', linestyle='--', label='25% Percentile')\n    plt.axvline(percentile_75, color='blue', linestyle='--', label='75% Percentile')\n    \n    plt.title(f'Distribution of {var}')\n    plt.xlabel(var)\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graphs we can see that \n- the average age is a little less than 35. Most of the people are in the age bracket 29~37.\n- The average income for the drivers is a little over 50000. Most of them are earning between 37K to 75K.\n- The average time a driver is associated with the company is 500 days. Most people stay betweeen 200 to 1000 days.","metadata":{}},{"cell_type":"markdown","source":"Suggestions:\n- The company can have a day wise incentive for the drivers. The more time they are associated with the company the more rewards/ money they get. This can reduce the churn for the drivers.","metadata":{}},{"cell_type":"code","source":"city_driver_count= aggregated_data['City'].value_counts()\nplt.figure()\ncity_driver_count.plot(kind='bar')\nplt.title('Number of Drivers in Each City')\nplt.xlabel('City')\nplt.ylabel('Number of Drivers')\nplt.xticks(rotation=45)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The company can try to reachout to more peolpe in the cities with less drivers, and provide them with more incentives so increase driver induction in the company. It will help keep a balance when the churn is high.","metadata":{}},{"cell_type":"markdown","source":"## Bivariate Analysis ","metadata":{}},{"cell_type":"code","source":"\n# Scatter plot for Age vs. Income\nplt.figure(figsize=(10, 6))\n\n# Scatter plot\nsns.scatterplot(data=aggregated_data, x='Age', y='Income')\n\n# Calculate and add average lines\naverage_age = aggregated_data['Age'].mean()\naverage_income = aggregated_data['Income'].mean()\n\nplt.axhline(average_income, color='red', linestyle='--', label='Average Income')\nplt.axvline(average_age, color='blue', linestyle='--', label='Average Age')\n\n\nplt.title('Scatter Plot: Age vs. Income')\nplt.xlabel('Age')\nplt.ylabel('Income')\nplt.legend()\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_columns = aggregated_data.select_dtypes(include=['int64', 'float64'])\n\n# Calculate the correlation matrix\ncorr_matrix = numeric_columns.corr()\n\n# Create a heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix Heatmap')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notable Correlations:\n\n- Income and Grade: There is a strong positive correlation of 0.74 between Income and Grade, suggesting that as one's grade increases, their income tends to increase as well.\n- Joining Designation and Grade: This pair also has a strong positive correlation (0.71), indicating a likely trend that individuals with a higher initial designation tend to reach higher grades.\n- Total Business Value and several factors: Total Business Value has moderately strong positive correlations with Income (0.38), Joining Designation (0.38), and Grade (0.38). This suggests that higher income, higher joining designation, and higher grade are associated with higher business values generated.\n- Target and several factors: The variable 'target' has a moderately strong negative correlation with Total Business Value (-0.38) and Quarterly Rating (-0.51), indicating that as the business value and quarterly ratings increase, the likelihood of hitting the target decreases (or vice versa). This could suggest that the targets might be set higher for individuals with higher business values or ratings.\n\nWeak or No Correlation: Several variables such as Driver_ID, Age, Gender, and Education Level show very little to no correlation with other variables, indicated by the colors close to white. This means that these factors do not have a strong linear relationship with the others in the dataset.","metadata":{}},{"cell_type":"code","source":"(aggregated_data['target']).value_counts().plot(kind='pie', figsize=(4, 4), colors=['darkcyan','red'], autopct='%1.0f%%')\n\nprint('=' * 30)\nprint((aggregated_data['target']).value_counts())\nprint('=' * 30)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The pie plot shows there is significant imbalance in the dataset. We will create the base models and then i will try to handle the imbalance and check whether the model performance improves after addressing it.","metadata":{}},{"cell_type":"markdown","source":"## Preparing the DataSet for ML-Model","metadata":{}},{"cell_type":"markdown","source":"Scaling the data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n\n# List of columns to standardize- Mostly numerical couloms\ncolumns_to_standardize = ['Driver_ID', 'Age', 'Education_Level', 'Income', 'Joining Designation', 'Grade',\n                           'Total Business Value', 'Quarterly Rating', 'Rating_Increased',\n                           'Income_Increased', 'total_num_of_days']\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# Fit the scaler on the selected columns and transform them\naggregated_data[columns_to_standardize] = scaler.fit_transform(aggregated_data[columns_to_standardize])\n\n# Now, the specified columns in 'aggregated_data' are standardized\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One hot encoding of the city column","metadata":{}},{"cell_type":"code","source":"aggregated_data= pd.get_dummies(aggregated_data, columns=['City'])\n\n\n# Print the resulting DataFrame\nprint(aggregated_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking the data before splitting","metadata":{}},{"cell_type":"code","source":"aggregated_data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggregated_data.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the data","metadata":{}},{"cell_type":"code","source":"# Split your data into training and testing sets\nX = aggregated_data.drop(columns=['target', 'Dateofjoining', 'LastWorkingDate'])\ny = aggregated_data['target']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a Random Forest Classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Fit the classifier on the training data\nrf_classifier.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = rf_classifier.predict(X_test)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\n\n# Create an XGBoost Classifier with scale_pos_weight\nxgb_classifier = xgb.XGBClassifier(scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum())\n\n# Fit the classifier on the training data\nxgb_classifier.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = xgb_classifier.predict(X_test)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The overall accuracy in both the models is pretty good at 88%. Lets see if we can improve the performance by impleminting SMOTE.","metadata":{}},{"cell_type":"markdown","source":"### Imbalance treatment","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nX = aggregated_data.drop(columns=['target', 'Dateofjoining', 'LastWorkingDate'])\ny = aggregated_data['target']\n\n# Initialize the SMOTE object\nsmote = SMOTE(random_state=42)\n\n# Apply SMOTE to the dataset to balance the classes\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Create a new DataFrame with the resampled data\nresampled_data = pd.concat([X_resampled, y_resampled], axis=1)\n\n# Check the class distribution after applying SMOTE\nprint(\"Class distribution after SMOTE:\\n\", resampled_data['target'].value_counts())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating the train test split from the SMOTE dataset","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\n# Create a Random Forest Classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Fit the classifier on the training data\nrf_classifier.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = rf_classifier.predict(X_test)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_classifier= xgb.XGBClassifier(\n    random_state=42  # Set the random seed for reproducibility\n)\n\nxgb_classifier.fit(X_train, y_train)\ny_pred = xgb_classifier.predict(X_test)\nprint(classification_report(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random forests performed better when applied the imbalance tratment. Now lets perform hyper parameter testting on it and see if i can improve the accuracy.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# Initialize StratifiedKFold with 5 folds\nstratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\n# Initialize lists to store evaluation results\nclassification_reports = []\n\n# Perform Stratified K-Fold cross-validation\nfor train_index, test_index in stratified_kfold.split(X_resampled, y_resampled):\n    X_train, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n    y_train, y_test = y_resampled.iloc[train_index], y_resampled.iloc[test_index]\n\n    # Fit the classifier on the training data\n    rf_classifier.fit(X_train, y_train)\n\n    # Make predictions on the test data\n    y_pred = rf_classifier.predict(X_test)\n\n    # Evaluate the model and store the classification report\n    classification_reports.append(classification_report(y_test, y_pred))\n\n# Print the classification reports for each fold\nfor i, report in enumerate(classification_reports, 1):\n    print(f\"Fold {i} Classification Report:\")\n    print(report)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfpr, tpr, _ = roc_curve(y_test, rf_classifier.predict_proba(X_test)[:, 1])\nroc_auc = auc(fpr, tpr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classification_report_str = classification_report(y_test, y_pred)\nconfusion_matrix_arr = confusion_matrix(y_test, y_pred)\n\n# Print the Classification Report and Confusion Matrix\nprint(\"Classification Report:\")\nprint(classification_report_str)\n\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix_arr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}